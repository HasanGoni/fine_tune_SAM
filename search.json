[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "fine_tune_SAM",
    "section": "",
    "text": "This file will become your README and also the index of your documentation.",
    "crumbs": [
      "fine_tune_SAM"
    ]
  },
  {
    "objectID": "index.html#install",
    "href": "index.html#install",
    "title": "fine_tune_SAM",
    "section": "Install",
    "text": "Install\npip install fine_tune_SAM",
    "crumbs": [
      "fine_tune_SAM"
    ]
  },
  {
    "objectID": "index.html#how-to-use",
    "href": "index.html#how-to-use",
    "title": "fine_tune_SAM",
    "section": "How to use",
    "text": "How to use\nFill me in please! Don’t forget code examples:\n\n1+1\n\n2",
    "crumbs": [
      "fine_tune_SAM"
    ]
  },
  {
    "objectID": "data_preparation.html",
    "href": "data_preparation.html",
    "title": "Dataset Preparation",
    "section": "",
    "text": "datset preparation\n\n\nim_path = Path(Path.cwd().parent/'data/patch_images')\nmsk_path = Path(Path.cwd().parent/'data/patch_masks')\nim_path_tst = Path(Path.cwd().parent/'data/test_patch_images')\nmsk_path_tst = Path(Path.cwd().parent/'data/test_patch_masks')\n\n\nsource\n\nget_dataset\n\n get_dataset (im_path:Union[pathlib.Path,str],\n              msk_path:Union[pathlib.Path,str])\n\nCreate a dataset from image and mask path\n\n\nDataset from own computer\n\n#test_dataset = get_dataset(\n    #im_path_tst,\n    #msk_path_tst,\n\n#)\n#test_dataset\n\n image has 1725 images\n masks has 1725 images\n\n\nDataset({\n    features: ['image', 'label'],\n    num_rows: 1725\n})\n\n\n\n#dataset = get_dataset(\n    #im_path,\n    #msk_path\n\n#)\n#dataset\n\n image has 1642 images\n masks has 1642 images\n\n\nDataset({\n    features: ['image', 'label'],\n    num_rows: 1642\n})\n\n\n\n#test_eq(len(test_dataset['image']), len(test_dataset['label']))\n\n\n#test_eq(len(dataset['image']), len(dataset['label']))\n\n\n#dataset_dict=DatasetDict({'train':dataset, 'test':test_dataset})\n\n\n#test_dataset.to_parquet(Path.cwd().parent/'data/test_patch_dataset.parquet')\n\n\n\n\n84715121\n\n\n\nsource\n\nshow_dataset\n\n show_dataset (dataset)\n\n\n#show_dataset(dataset)\n\n dataset index will be visualized: 224\n\n\n\n\n\n\n\n\n\n\n\n\nIn case of download data from huggingface\n\nfrom datasets import load_dataset\n\ndataset = load_dataset(\"hasangoni/Electron_microscopy_dataset\")\n\nDownloading readme:   0%|          | 0.00/608 [00:00&lt;?, ?B/s]Downloading readme: 100%|##########| 608/608 [00:00&lt;00:00, 4.37MB/s]\n\n\nDownloading data:   0%|          | 0.00/80.6M [00:00&lt;?, ?B/s]\n\nDownloading data:  13%|#3        | 10.5M/80.6M [00:00&lt;00:02, 29.9MB/s]\n\nDownloading data:  26%|##6       | 21.0M/80.6M [00:00&lt;00:01, 51.0MB/s]\n\nDownloading data:  39%|###9      | 31.5M/80.6M [00:00&lt;00:00, 53.1MB/s]\n\nDownloading data:  52%|#####2    | 41.9M/80.6M [00:00&lt;00:00, 56.4MB/s]\n\nDownloading data:  65%|######5   | 52.4M/80.6M [00:00&lt;00:00, 63.8MB/s]\n\nDownloading data:  91%|#########1| 73.4M/80.6M [00:01&lt;00:00, 65.1MB/s]\n\nDownloading data: 100%|##########| 80.6M/80.6M [00:01&lt;00:00, 66.0MB/s]Downloading data: 100%|##########| 80.6M/80.6M [00:01&lt;00:00, 58.6MB/s]\n\n\nDownloading data:   0%|          | 0.00/84.6M [00:00&lt;?, ?B/s]\n\nDownloading data:  12%|#2        | 10.5M/84.6M [00:00&lt;00:01, 42.9MB/s]\n\nDownloading data:  25%|##4       | 21.0M/84.6M [00:00&lt;00:01, 52.1MB/s]\n\nDownloading data:  50%|####9     | 41.9M/84.6M [00:00&lt;00:00, 75.4MB/s]\n\nDownloading data:  62%|######1   | 52.4M/84.6M [00:01&lt;00:00, 48.9MB/s]\n\nDownloading data:  74%|#######4  | 62.9M/84.6M [00:01&lt;00:00, 51.8MB/s]\n\nDownloading data:  87%|########6 | 73.4M/84.6M [00:01&lt;00:00, 53.4MB/s]\n\nDownloading data:  99%|#########9| 83.9M/84.6M [00:01&lt;00:00, 45.7MB/s]Downloading data: 100%|##########| 84.6M/84.6M [00:01&lt;00:00, 49.1MB/s]\nGenerating train split:   0%|          | 0/1642 [00:00&lt;?, ? examples/s]Generating train split: 100%|##########| 1642/1642 [00:00&lt;00:00, 12469.15 examples/s]Generating train split: 100%|##########| 1642/1642 [00:00&lt;00:00, 12402.03 examples/s]\nGenerating test split:   0%|          | 0/1725 [00:00&lt;?, ? examples/s]Generating test split: 100%|##########| 1725/1725 [00:00&lt;00:00, 12490.33 examples/s]Generating test split: 100%|##########| 1725/1725 [00:00&lt;00:00, 12385.07 examples/s]\n\n\n\ndataset\n\nDatasetDict({\n    train: Dataset({\n        features: ['image', 'label'],\n        num_rows: 1642\n    })\n})\n\n\n\ndef show_hf_dataset(\n        dataset:Dataset,\n        idx:Union[int, None]=None,\n        split:str='train'\n        ):\n    \"Show hugging face random index\"\n\n    if idx is None:\n        idx = np.random.randint(0, len(dataset[split]))\n\n    print(f' dataset index will be visualized: {idx}')\n    im_ = dataset[split]['image'][idx]\n    msk_ = dataset[split]['label'][idx]\n    fig, ax = plt.subplots(\n        1, 2, figsize=(10, 5)\n    )\n    ax[0].imshow(im_)\n    ax[1].imshow(msk_)\n\n\nshow_hf_dataset(dataset)\n\n dataset index will be visualized: 189\n\n\n\n\n\n\n\n\n\n\nsource\n\nget_bounding_box\n\n get_bounding_box (ground_truth_map)\n\n\nCareful here to make difference between torch Dataset and datsets.Dataset\n\n\nsource\n\n\nSAMDataset\n\n SAMDataset (dataset, processors)\n\nCreating dataset for SAM training\n\ntrain_ds = SAMDataset(\n    dataset=dataset, \n    processors=processor)\n\n\n#example = train_ds[0]\n#for k,v in example.items():\n    #print(f'{k}: {v.shape}')\n\n(256, 256) &lt;class 'numpy.ndarray'&gt;\npixel_values: torch.Size([3, 1024, 1024])\noriginal_sizes: torch.Size([2])\nreshaped_input_sizes: torch.Size([2])\ninput_boxes: torch.Size([1, 4])\nground_truth_mask: (256, 256)\n\n\n\n#train_dl = DataLoader(\n    #train_ds, \n    #batch_size=2, \n    #shuffle=True\n  #)\n\n\n#batch = next(iter(train_dl))\n#for k,v in batch.items():\n    #print(k,v.shape)\n\n(256, 256) &lt;class 'numpy.ndarray'&gt;\n(256, 256) &lt;class 'numpy.ndarray'&gt;\npixel_values torch.Size([2, 3, 1024, 1024])\noriginal_sizes torch.Size([2, 2])\nreshaped_input_sizes torch.Size([2, 2])\ninput_boxes torch.Size([2, 1, 4])\nground_truth_mask torch.Size([2, 256, 256])",
    "crumbs": [
      "Dataset Preparation"
    ]
  },
  {
    "objectID": "patch_dataset.html",
    "href": "patch_dataset.html",
    "title": "Dataset Patchify",
    "section": "",
    "text": "dataset will be patched here from whole side images\n\n\nReading train images and masks from folder\nTraining and test image and mask can be downloaded from the following command - curl https://documents.epfl.ch/groups/c/cv/cvlab-unit/www/data/%20ElectronMicroscopy_Hippocampus/training_groundtruth.tif &gt; mask_name.tif - curl https://documents.epfl.ch/groups/c/cv/cvlab-unit/www/data/%20ElectronMicroscopy_Hippocampus/training.tif&gt; train_images.tif - curl https://documents.epfl.ch/groups/c/cv/cvlab-unit/www/data/%20ElectronMicroscopy_Hippocampus/testing_groundtruth.tif&gt; test_mask.tif - curl https://documents.epfl.ch/groups/c/cv/cvlab-unit/www/data/%20ElectronMicroscopy_Hippocampus/testing.tif&gt; test_images.tif\n\nroot_path = Path.cwd().parent\n\nPath('/home/hasan/workspace/git_data/fine_tune_SAM')\n\n\n\n### test images\n\n\nbig_img_tst = tifffile.imread(test_imgs)\nbig_msk_tst = tifffile.imread(test_masks)\nprint(f' image tif has a shape and mask tif has a shape of {big_img_tst.shape} and {big_msk_tst.shape} respectively')\n\n image tif has a shape and mask tif has a shape of (165, 768, 1024) and (165, 768, 1024) respectively\n\n\n\n### train images\n\n\n\nTesting image and masks\n\nsource\n\nshow_rand_img\n\n show_rand_img (idx:int, im_path:str, msk_path:str)\n\nShow random mask and image from together from path\n\n\n\n\nType\nDetails\n\n\n\n\nidx\nint\nin case of None, a random image is chosen\n\n\nim_path\nstr\n\n\n\nmsk_path\nstr\n\n\n\n\nNotebook is here\n\n\n\nSaving the images in a single folder\n\nfor this project this part is not necessary.\nI am doing it so that we can use patchify in any projects\nJust tell where the full images are available, tell patch size and step size and it will created the patches with same name of the images, ony another index will be created to tell which patch number is that\n\n\nfor i in tqdm(range(len(big_img_tst))):\n    cv2.imwrite(f'{actual_im_path_tst}/img_{i}.png', big_img_tst[i])\n    cv2.imwrite(f'{actual_msk_path_tst}/img_{i}.png', big_msk_tst[i])\n\n100%|██████████| 165/165 [00:02&lt;00:00, 60.49it/s]\n\n\n\nPath(actual_im_path_tst).ls()\n\n(#165) [Path('/home/hasan/workspace/data/microscopy_data/test_images/img_51.png'),Path('/home/hasan/workspace/data/microscopy_data/test_images/img_98.png'),Path('/home/hasan/workspace/data/microscopy_data/test_images/img_133.png'),Path('/home/hasan/workspace/data/microscopy_data/test_images/img_107.png'),Path('/home/hasan/workspace/data/microscopy_data/test_images/img_16.png'),Path('/home/hasan/workspace/data/microscopy_data/test_images/img_150.png'),Path('/home/hasan/workspace/data/microscopy_data/test_images/img_158.png'),Path('/home/hasan/workspace/data/microscopy_data/test_images/img_145.png'),Path('/home/hasan/workspace/data/microscopy_data/test_images/img_48.png'),Path('/home/hasan/workspace/data/microscopy_data/test_images/img_40.png')...]\n\n\n\nfor i in tqdm(range(len(big_img))):\n    cv2.imwrite(f'{actual_im_path}/img_{i}.png', big_img[i])\n    cv2.imwrite(f'{actual_msk_path}/img_{i}.png', big_msk[i])\n\n100%|██████████| 165/165 [00:02&lt;00:00, 61.58it/s]\n\n\n\nshow_rand_img(idx=None, im_path=actual_im_path, msk_path=actual_msk_path)\n\n\n\n\n\n\n\n\n\n\nCreating patches from the folder\n\npatch_img_path_tst=Path(r'/home/hasan/workspace/data/microscopy_data/test_patch_images')\npatch_img_path_tst.mkdir(exist_ok=True, parents=True)\npatch_msk_path_tst=Path(r'/home/hasan/workspace/data/microscopy_data/test_patch_masks')\npatch_msk_path_tst.mkdir(exist_ok=True, parents=True)\n\n\nsource\n\npatch_img_and_mask\n\n patch_img_and_mask (im_path:Union[pathlib.Path,str],\n                     msk_path:Union[pathlib.Path,str],\n                     patch_im_path:Union[pathlib.Path,str],\n                     patch_msk_path:Union[pathlib.Path,str],\n                     PATCH_SIZE:int=256, STEP:int=256)\n\nCreate patch images and masks from original images and masks\n\n\n\ndelete patches with empty masks\n\nThere is possible some patch has no mask, we want to delete those mask\n\n\nfor i in tqdm(patch_msk_path_tst.ls()):\n    name_ = Path(i).name\n    img = cv2.imread(i.as_posix(), cv2.IMREAD_GRAYSCALE)\n    if not img.max() &gt; 0:\n        i.unlink()\n        im_ = Path(f'{patch_img_path_tst}/{name_}')\n        im_.unlink()\n\n100%|██████████| 1980/1980 [00:00&lt;00:00, 2423.80it/s]\n\n\n\nfor i in tqdm(patch_msk_path.ls()):\n    name_ = Path(i).name\n    img = cv2.imread(i.as_posix(), cv2.IMREAD_GRAYSCALE)\n    if not img.max() &gt; 0:\n        i.unlink()\n        im_ = Path(f'{patch_img_path}/{name_}')\n        im_.unlink()\n\n100%|██████████| 1980/1980 [00:00&lt;00:00, 2697.89it/s]\n\n\n\nshow_rand_img(\n    im_path=patch_img_path_tst, \n    msk_path=patch_msk_path_tst, \n    idx=None)\n\n\n\n\n\n\n\n\n\nshow_rand_img(\n    im_path=patch_img_path, \n    msk_path=patch_msk_path, \n    idx=None)\n\n\n\n\n\n\n\n\n\none can go further and check what is the percentage of mask, if mask has a defined percentage of pixels, then keep the patch, otherwise delete it\nbut for me, right now i will further with modelling\n\n\nimport nbdev; nbdev.nbdev_export()",
    "crumbs": [
      "Dataset Patchify"
    ]
  },
  {
    "objectID": "inference_with_medsam.html",
    "href": "inference_with_medsam.html",
    "title": "MedSAM inference",
    "section": "",
    "text": "MedSAM inference will be done here\n\n\nfrom datasets import load_dataset\n\n\ndataset = load_dataset(\"nielsr/breast-cancer\", split=\"train\")\n\n\n\n\n\n\n\n\n\n\n\nidx = 10\n\nimage = dataset[idx][\"image\"]\nlabel = dataset[idx][\"label\"]\nmsk = np.array(label)\nan_img = overlay_mask_border_on_image_frm_img(\n    image, msk,\n)\nshow_(an_img)\n#msk.shape, image.size\n\n\n\n\n\n\n\n\n\ncntrs = find_contours_binary(msk.astype(np.uint8))[0]\nx, y, w, h = frm_cntr_to_bbox(cntrs)\n\n\ndef get_model():\n    model = SamModel.from_pretrained(\"wanglab/medsam-vit-base\")\n    return model\n\n\nsource\n\nget_bounding_box\n\n get_bounding_box (ground_truth_map:numpy.ndarray)\n\nGet bounding box from mask\n\nsource\n\n\nget_prediction\n\n get_prediction (model:transformers.models.sam.modeling_sam.SamModel,\n                 model_name:str, image:PIL.Image.Image, boxes:List[int],\n                 device:Optional[str]=None, threshold:float=0.5)\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nmodel\nSamModel\n\n\n\n\nmodel_name\nstr\n\ncheckpoint in hugggingface\n\n\nimage\nImage\n\n\n\n\nboxes\nList\n\n\n\n\ndevice\nOptional\nNone\n\n\n\nthreshold\nfloat\n0.5\n\n\n\nReturns\nTuple\n\n\n\n\n\n\nmodel = get_model()\n\n\npreds_ = get_prediction(\n    model=model,\n    model_name='wanglab/medsam-vit-base',\n    image=image,\n    boxes=[get_bounding_box(msk)],\n    device='cpu',\n    threshold=0.9\n\n)\n\n\nan_img = overlay_mask_border_on_image_frm_img(image, preds_)\nshow_(an_img)",
    "crumbs": [
      "MedSAM inference"
    ]
  },
  {
    "objectID": "core.html",
    "href": "core.html",
    "title": "core",
    "section": "",
    "text": "source\n\nfoo\n\n foo ()",
    "crumbs": [
      "core"
    ]
  },
  {
    "objectID": "data_prep_from_hf.html",
    "href": "data_prep_from_hf.html",
    "title": "Dataset preparation from HF",
    "section": "",
    "text": "Data will be downloaded from hugging face and then will be processed to get the data in the format we want.\n\n\n#from cv_tools.core import *\n\n\ndataset = load_dataset(\"nielsr/breast-cancer\", split=\"train\")\ndataset\n\nDataset({\n    features: ['image', 'label'],\n    num_rows: 130\n})\n\n\n\nimg = dataset[0]['image']\nmsk = dataset[0]['label']\nimg.size, img.mode, msk.size, msk.mode\n\n((256, 256), 'RGB', (256, 256), 'I')\n\n\n\nfrom datasets import load_dataset\n\n\ntraining_dataset = load_dataset(\n    \"hasangoni/Electron_microscopy_dataset\",\n    split=\"train\"\n    )\nvalidation_dataset = load_dataset(\n    \"hasangoni/Electron_microscopy_dataset\",\n    split=\"test\"\n    )\n\n\ntraining_dataset\n\nDataset({\n    features: ['image', 'label'],\n    num_rows: 1642\n})\n\n\n\nsource\n\nget_bounding_box\n\n get_bounding_box (ground_truth_map:numpy.ndarray)\n\nGet bounding box coordinates from mask image\n\n\n\n\nType\nDetails\n\n\n\n\nground_truth_map\nndarray\nmask image type cv2\n\n\n\ntesing get_bounding_box\n\nsource\n\n\nSAMDataset\n\n SAMDataset (dataset:torch.utils.data.dataset.Dataset,\n             processor:transformers.models.sam.processing_sam.SamProcessor\n             )\n\nCreating dataset for SAM Training\n\n\n\n\nType\nDetails\n\n\n\n\ndataset\nDataset\npytorch dataset\n\n\nprocessor\nSamProcessor\nhf model processor\n\n\n\n\n\nCreating pytorch dataset\n\nprocessor = SamProcessor.from_pretrained('facebook/sam-vit-base')\ntrain_dataset = SAMDataset(\n  dataset=training_dataset, \n  processor=processor)\nval_dataset = SAMDataset(\n  dataset=validation_dataset, \n  processor=processor)\n\n\ntrn_ = np.transpose(train_dataset[0]['pixel_values'].to('cpu').numpy(), (1,2,0))\nval_ = np.transpose(val_dataset[0]['pixel_values'].to('cpu').numpy(), (1,2,0))\n#show_(trn_)\n\n\ntrain_dataloader = DataLoader(\n                            train_dataset, \n                            batch_size=2,\n                            shuffle=True)\nval_dataloader = DataLoader(\n                            val_dataset, \n                            batch_size=2,\n                            shuffle=False)\n\n\nexample = train_dataset[0]\nfor k,v in example.items():\n  print(k,v.shape)\n     \nmodel = SamModel.from_pretrained(\"facebook/sam-vit-base\")\n\n# make sure we only compute gradients for mask decoder\nfor name, param in model.named_parameters():\n  if name.startswith(\"vision_encoder\") or name.startswith(\"prompt_encoder\"):\n    param.requires_grad_(False)\n\npixel_values torch.Size([3, 1024, 1024])\noriginal_sizes torch.Size([2])\nreshaped_input_sizes torch.Size([2])\ninput_boxes torch.Size([1, 4])\nground_truth_mask (256, 256)\n\n\n\n\nCreating pytorch pytorch dataloader\n\ntrain_dataloader = DataLoader(\n    train_ds, \n    batch_size=2, \n    shuffle=True)\nval_dataloader = DataLoader(\n    val_ds, \n    batch_size=2, \n    shuffle=False)\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[149], line 2\n      1 train_dataloader = DataLoader(\n----&gt; 2     train_ds, \n      3     batch_size=2, \n      4     shuffle=True)\n      5 val_dataloader = DataLoader(\n      6     val_ds, \n      7     batch_size=2, \n      8     shuffle=False)\n\nNameError: name 'train_ds' is not defined\n\n\n\ntesting pytorch dataloader\n\nbatch = next(iter(train_dataloader))\nfor k,v in batch.items():\n  print(k,v.shape)\n\npixel_values torch.Size([2, 3, 1024, 1024])\noriginal_sizes torch.Size([2, 2])\nreshaped_input_sizes torch.Size([2, 2])\ninput_boxes torch.Size([2, 1, 4])\nground_truth_mask torch.Size([2, 256, 256])\n\n\n\n\nloading Model\n\nmodel = SamModel.from_pretrained('facebook/sam-vit-base')\n\n\n# make sure we only compute gradients for mask decoder\nfor name, param in model.named_parameters():\n  if name.startswith(\"vision_encoder\") or name.startswith(\"prompt_encoder\"):\n    param.requires_grad_(False)\n\n\nNUM_EPOCHS = 2\nT_0 = int(0.5 * NUM_EPOCHS)\nITERS = len(train_dataloader)\n\n\noptimizer = AdamW(\n    model.mask_decoder.parameters(),\n    lr=0.001,\n    weight_decay=0.0001)\n\n\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\ndevice= \"cpu\"\n# in case of very small gpu memory, like me then use cpu\n#device = \"cpu\"\nmodel.to(device)\nscheduler = CosineAnnealingWarmRestarts(\n   T_0=T_0,\n   optimizer=optimizer, \n   eta_min=0.00001)\n\n\ndevice='cpu'\n\n\n\nvalidate\n\n validate (model:transformers.models.sam.modeling_sam.SamModel,\n           dataloader:torch.utils.data.dataloader.DataLoader, loss_fn:&lt;mod\n           ule'monai.losses.dice'from'/opt/hostedtoolcache/Python/3.10.14/\n           x64/lib/python3.10/site-packages/monai/losses/dice.py'&gt;,\n           device:str='cpu')\n\n\nnum_epochs = 100\n\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nmodel.to(device)\n\nseg_loss = monai.losses.DiceCELoss(\n    sigmoid=True, \n    squared_pred=True, \n    reduction='mean')\nmodel.train()\nfor epoch in range(num_epochs):\n    epoch_losses = []\n    for batch in tqdm(train_dataloader):\n      # forward pass\n      print(batch['pixel_values'].shape)\n      outputs = model(pixel_values=batch[\"pixel_values\"].to(device),\n                      input_boxes=batch[\"input_boxes\"].to(device),\n                      multimask_output=False)\n      print(f'outputs shape {outputs.pred_masks.shape}')\n\n      # compute loss\n      predicted_masks = outputs.pred_masks.squeeze(1)\n      print(f'predicted_masks shape {predicted_masks.shape}') \n      ground_truth_masks = batch[\"ground_truth_mask\"].float().to(device)\n      print(f'ground_truth_masks shape {ground_truth_masks.shape}')\n      loss = seg_loss(predicted_masks, ground_truth_masks.unsqueeze(1))\n\n      # backward pass (compute gradients of parameters w.r.t. loss)\n      optimizer.zero_grad()\n      loss.backward()\n\n      # optimize\n      optimizer.step()\n      epoch_losses.append(loss.item())\n\n    print(f'EPOCH: {epoch}')\n    print(f'Mean Training Loss: {mean(epoch_losses)}')\n    validation_loss = validate(\n        model=model, \n        dataloader=val_dataloader, \n        loss_fn=seg_loss, \n        device=device)\n\n    print(f'Validation Loss: {validation_loss}')\n\n  0%|          | 0/65 [00:00&lt;?, ?it/s]\n\n\ntorch.Size([2, 3, 1024, 1024])\n\n\n  0%|          | 0/65 [00:03&lt;?, ?it/s]\n\n\n\n---------------------------------------------------------------------------\nKeyboardInterrupt                         Traceback (most recent call last)\nCell In[44], line 16\n     13 for batch in tqdm(train_dataloader):\n     14   # forward pass\n     15   print(batch['pixel_values'].shape)\n---&gt; 16   outputs = model(pixel_values=batch[\"pixel_values\"].to(device),\n     17                   input_boxes=batch[\"input_boxes\"].to(device),\n     18                   multimask_output=False)\n     19   print(f'outputs shape {outputs.pred_masks.shape}')\n     21   # compute loss\n\nFile ~/miniconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1511, in Module._wrapped_call_impl(self, *args, **kwargs)\n   1509     return self._compiled_call_impl(*args, **kwargs)  # type: ignore[misc]\n   1510 else:\n-&gt; 1511     return self._call_impl(*args, **kwargs)\n\nFile ~/miniconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1520, in Module._call_impl(self, *args, **kwargs)\n   1515 # If we don't have any hooks, we want to skip the rest of the logic in\n   1516 # this function, and just call forward.\n   1517 if not (self._backward_hooks or self._backward_pre_hooks or self._forward_hooks or self._forward_pre_hooks\n   1518         or _global_backward_pre_hooks or _global_backward_hooks\n   1519         or _global_forward_hooks or _global_forward_pre_hooks):\n-&gt; 1520     return forward_call(*args, **kwargs)\n   1522 try:\n   1523     result = None\n\nFile ~/miniconda3/lib/python3.11/site-packages/transformers/models/sam/modeling_sam.py:1358, in SamModel.forward(self, pixel_values, input_points, input_labels, input_boxes, input_masks, image_embeddings, multimask_output, attention_similarity, target_embedding, output_attentions, output_hidden_states, return_dict, **kwargs)\n   1355 vision_hidden_states = None\n   1357 if pixel_values is not None:\n-&gt; 1358     vision_outputs = self.vision_encoder(\n   1359         pixel_values,\n   1360         output_attentions=output_attentions,\n   1361         output_hidden_states=output_hidden_states,\n   1362         return_dict=return_dict,\n   1363     )\n   1364     image_embeddings = vision_outputs[0]\n   1366     if output_hidden_states:\n\nFile ~/miniconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1511, in Module._wrapped_call_impl(self, *args, **kwargs)\n   1509     return self._compiled_call_impl(*args, **kwargs)  # type: ignore[misc]\n   1510 else:\n-&gt; 1511     return self._call_impl(*args, **kwargs)\n\nFile ~/miniconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1520, in Module._call_impl(self, *args, **kwargs)\n   1515 # If we don't have any hooks, we want to skip the rest of the logic in\n   1516 # this function, and just call forward.\n   1517 if not (self._backward_hooks or self._backward_pre_hooks or self._forward_hooks or self._forward_pre_hooks\n   1518         or _global_backward_pre_hooks or _global_backward_hooks\n   1519         or _global_forward_hooks or _global_forward_pre_hooks):\n-&gt; 1520     return forward_call(*args, **kwargs)\n   1522 try:\n   1523     result = None\n\nFile ~/miniconda3/lib/python3.11/site-packages/transformers/models/sam/modeling_sam.py:1046, in SamVisionEncoder.forward(self, pixel_values, output_attentions, output_hidden_states, return_dict)\n   1041     layer_outputs = self._gradient_checkpointing_func(\n   1042         layer_module.__call__,\n   1043         hidden_states,\n   1044     )\n   1045 else:\n-&gt; 1046     layer_outputs = layer_module(hidden_states, output_attentions=output_attentions)\n   1048 hidden_states = layer_outputs[0]\n   1050 if output_attentions:\n\nFile ~/miniconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1511, in Module._wrapped_call_impl(self, *args, **kwargs)\n   1509     return self._compiled_call_impl(*args, **kwargs)  # type: ignore[misc]\n   1510 else:\n-&gt; 1511     return self._call_impl(*args, **kwargs)\n\nFile ~/miniconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1520, in Module._call_impl(self, *args, **kwargs)\n   1515 # If we don't have any hooks, we want to skip the rest of the logic in\n   1516 # this function, and just call forward.\n   1517 if not (self._backward_hooks or self._backward_pre_hooks or self._forward_hooks or self._forward_pre_hooks\n   1518         or _global_backward_pre_hooks or _global_backward_hooks\n   1519         or _global_forward_hooks or _global_forward_pre_hooks):\n-&gt; 1520     return forward_call(*args, **kwargs)\n   1522 try:\n   1523     result = None\n\nFile ~/miniconda3/lib/python3.11/site-packages/transformers/models/sam/modeling_sam.py:939, in SamVisionLayer.forward(self, hidden_states, output_attentions)\n    936     height, width = hidden_states.shape[1], hidden_states.shape[2]\n    937     hidden_states, padding_shape = self.window_partition(hidden_states, self.window_size)\n--&gt; 939 hidden_states, attn_weights = self.attn(\n    940     hidden_states=hidden_states,\n    941     output_attentions=output_attentions,\n    942 )\n    943 # Reverse window partition\n    944 if self.window_size &gt; 0:\n\nFile ~/miniconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1511, in Module._wrapped_call_impl(self, *args, **kwargs)\n   1509     return self._compiled_call_impl(*args, **kwargs)  # type: ignore[misc]\n   1510 else:\n-&gt; 1511     return self._call_impl(*args, **kwargs)\n\nFile ~/miniconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1520, in Module._call_impl(self, *args, **kwargs)\n   1515 # If we don't have any hooks, we want to skip the rest of the logic in\n   1516 # this function, and just call forward.\n   1517 if not (self._backward_hooks or self._backward_pre_hooks or self._forward_hooks or self._forward_pre_hooks\n   1518         or _global_backward_pre_hooks or _global_backward_hooks\n   1519         or _global_forward_hooks or _global_forward_pre_hooks):\n-&gt; 1520     return forward_call(*args, **kwargs)\n   1522 try:\n   1523     result = None\n\nFile ~/miniconda3/lib/python3.11/site-packages/transformers/models/sam/modeling_sam.py:842, in SamVisionAttention.forward(self, hidden_states, output_attentions)\n    839 attn_weights = (query * self.scale) @ key.transpose(-2, -1)\n    841 if self.use_rel_pos:\n--&gt; 842     attn_weights = self.add_decomposed_rel_pos(\n    843         attn_weights, query, self.rel_pos_h, self.rel_pos_w, (height, width), (height, width)\n    844     )\n    846 attn_weights = torch.nn.functional.softmax(attn_weights, dtype=torch.float32, dim=-1).to(query.dtype)\n    848 attn_probs = nn.functional.dropout(attn_weights, p=self.dropout, training=self.training)\n\nFile ~/miniconda3/lib/python3.11/site-packages/transformers/models/sam/modeling_sam.py:825, in SamVisionAttention.add_decomposed_rel_pos(self, attn, query, rel_pos_h, rel_pos_w, q_size, k_size)\n    823 attn = attn.reshape(batch_size, query_height, query_width, key_height, key_width)\n    824 attn = attn + rel_h[:, :, :, :, None] + rel_w[:, :, :, None, :]\n--&gt; 825 attn = attn.reshape(batch_size, query_height * query_width, key_height * key_width)\n    826 return attn\n\nKeyboardInterrupt: \n\n\n\n\n\n\npt_train\n\n pt_train (train_dataloader:torch.utils.data.dataloader.DataLoader,\n           model:transformers.models.sam.modeling_sam.SamModel,\n           optimizer:torch.optim.optimizer.Optimizer,\n           device:Optional[str]='cpu', epoch_n:int=2)\n\n\npt_train(\n    train_dataloader=train_dataloader,\n    model=model, \n    optimizer=optimizer, \n    device=device, \n    epoch_n=2)\n\nEpoch 1\n\n\n23it [08:41, 22.89s/it]\n\n\n\nThe Kernel crashed while executing code in the current cell or a previous cell. \n\nPlease review the code in the cell(s) to identify a possible cause of the failure. \n\nClick &lt;a href='https://aka.ms/vscodeJupyterKernelCrash'&gt;here&lt;/a&gt; for more info. \n\nView Jupyter &lt;a href='command:jupyter.viewOutput'&gt;log&lt;/a&gt; for further details.\n\n\n\n\n\n\nvalidate\n\n validate (model:transformers.models.sam.modeling_sam.SamModel,\n           dataloader:torch.utils.data.dataloader.DataLoader, loss_fn:&lt;mod\n           ule'monai.losses'from'/opt/hostedtoolcache/Python/3.10.14/x64/l\n           ib/python3.10/site-packages/monai/losses/__init__.py'&gt;,\n           device:str='cpu')\n\n*Validate the model using a validation dataloader.\nParameters: - model: The PyTorch model to validate. - dataloader: DataLoader for validation data. - loss_fn: Loss function used for validation. - device: Device to run validation on (‘cuda’ or ‘cpu’).*\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nmodel\nSamModel\n\nSAM model\n\n\ndataloader\nDataLoader\n\nTorch dataloader\n\n\nloss_fn\nmonai.losses\n\nMonai loss function\n\n\ndevice\nstr\ncpu\nwhether to use cpu or gpu\n\n\n\n\n\n\ntrain_and_validate\n\n train_and_validate (model:transformers.models.sam.modeling_sam.SamModel,\n                     num_epochs:int,\n                     optimizer:torch.optim.optimizer.Optimizer, scheduler:\n                     &lt;module'torch.optim.lr_scheduler'from'/opt/hostedtool\n                     cache/Python/3.10.14/x64/lib/python3.10/site-\n                     packages/torch/optim/lr_scheduler.py'&gt;, train_dataloa\n                     der:torch.utils.data.dataloader.DataLoader, val_datal\n                     oader:torch.utils.data.dataloader.DataLoader, loss_fn\n                     :&lt;module'monai.losses'from'/opt/hostedtoolcache/Pytho\n                     n/3.10.14/x64/lib/python3.10/site-\n                     packages/monai/losses/__init__.py'&gt;,\n                     device:str='cpu')\n\nTrain and validate a model with the given parameters.\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nmodel\nSamModel\n\nSAM model\n\n\nnum_epochs\nint\n\nNumber of epochs to train for\n\n\noptimizer\nOptimizer\n\nOptimizer to use\n\n\nscheduler\ntorch.optim.lr_scheduler\n\nLearning rate scheduler\n\n\ntrain_dataloader\nDataLoader\n\nDataLoader for training data\n\n\nval_dataloader\nDataLoader\n\nDataLoader for validation data\n\n\nloss_fn\nmonai.losses\n\nLoss function used for training\n\n\ndevice\nstr\ncpu\nDevice to train on (‘cuda’ or ‘cpu’)\n\n\n\n\n#model = SamModel.from_pretrained('facebook/sam-vit-base')\noptimizer = AdamW(\n    model.mask_decoder.parameters(),\n    lr=0.001,\n    weight_decay=0.0001)\n\nscheduler = CosineAnnealingWarmRestarts(\n   T_0=10,\n   T_mult=2,\n   optimizer=optimizer, \n   eta_min=0.00001)\nseg_loss = monai.losses.DiceCELoss()\ndevice='cpu'\n\n\ntrain_and_validate(\n    model=model,\n    num_epochs=2,\n    optimizer=optimizer,\n    scheduler=scheduler,\n    train_dataloader=train_dataloader,\n    val_dataloader=val_dataloader,\n    loss_fn=seg_loss,\n    device=device\n)\n\nEpoch 1/2:   0%|          | 1/821 [00:36&lt;8:21:37, 36.70s/it]\n\n\n\n---------------------------------------------------------------------------\nKeyboardInterrupt                         Traceback (most recent call last)\nCell In[161], line 1\n----&gt; 1 train_and_validate(\n      2     model=model,\n      3     num_epochs=2,\n      4     optimizer=optimizer,\n      5     scheduler=scheduler,\n      6     train_dataloader=train_dataloader,\n      7     val_dataloader=val_dataloader,\n      8     loss_fn=seg_loss,\n      9     device=device\n     10 )\n\nCell In[159], line 26, in train_and_validate(model, num_epochs, optimizer, scheduler, train_dataloader, val_dataloader, loss_fn, device)\n     20 progress_bar = tqdm(\n     21     train_dataloader, \n     22     desc=f'Epoch {epoch+1}/{num_epochs}', total=len(train_dataloader))\n     24 for batch in progress_bar:\n     25     # Forward pass\n---&gt; 26     outputs = model(pixel_values=batch[\"pixel_values\"].to(device),\n     27                     input_boxes=batch[\"input_boxes\"].to(device),\n     28                     multimask_output=False)\n     30     # Compute loss\n     31     predicted_masks = outputs.pred_masks.squeeze(1)\n\nFile ~/miniconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1511, in Module._wrapped_call_impl(self, *args, **kwargs)\n   1509     return self._compiled_call_impl(*args, **kwargs)  # type: ignore[misc]\n   1510 else:\n-&gt; 1511     return self._call_impl(*args, **kwargs)\n\nFile ~/miniconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1520, in Module._call_impl(self, *args, **kwargs)\n   1515 # If we don't have any hooks, we want to skip the rest of the logic in\n   1516 # this function, and just call forward.\n   1517 if not (self._backward_hooks or self._backward_pre_hooks or self._forward_hooks or self._forward_pre_hooks\n   1518         or _global_backward_pre_hooks or _global_backward_hooks\n   1519         or _global_forward_hooks or _global_forward_pre_hooks):\n-&gt; 1520     return forward_call(*args, **kwargs)\n   1522 try:\n   1523     result = None\n\nFile ~/miniconda3/lib/python3.11/site-packages/transformers/models/sam/modeling_sam.py:1358, in SamModel.forward(self, pixel_values, input_points, input_labels, input_boxes, input_masks, image_embeddings, multimask_output, attention_similarity, target_embedding, output_attentions, output_hidden_states, return_dict, **kwargs)\n   1355 vision_hidden_states = None\n   1357 if pixel_values is not None:\n-&gt; 1358     vision_outputs = self.vision_encoder(\n   1359         pixel_values,\n   1360         output_attentions=output_attentions,\n   1361         output_hidden_states=output_hidden_states,\n   1362         return_dict=return_dict,\n   1363     )\n   1364     image_embeddings = vision_outputs[0]\n   1366     if output_hidden_states:\n\nFile ~/miniconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1511, in Module._wrapped_call_impl(self, *args, **kwargs)\n   1509     return self._compiled_call_impl(*args, **kwargs)  # type: ignore[misc]\n   1510 else:\n-&gt; 1511     return self._call_impl(*args, **kwargs)\n\nFile ~/miniconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1520, in Module._call_impl(self, *args, **kwargs)\n   1515 # If we don't have any hooks, we want to skip the rest of the logic in\n   1516 # this function, and just call forward.\n   1517 if not (self._backward_hooks or self._backward_pre_hooks or self._forward_hooks or self._forward_pre_hooks\n   1518         or _global_backward_pre_hooks or _global_backward_hooks\n   1519         or _global_forward_hooks or _global_forward_pre_hooks):\n-&gt; 1520     return forward_call(*args, **kwargs)\n   1522 try:\n   1523     result = None\n\nFile ~/miniconda3/lib/python3.11/site-packages/transformers/models/sam/modeling_sam.py:1046, in SamVisionEncoder.forward(self, pixel_values, output_attentions, output_hidden_states, return_dict)\n   1041     layer_outputs = self._gradient_checkpointing_func(\n   1042         layer_module.__call__,\n   1043         hidden_states,\n   1044     )\n   1045 else:\n-&gt; 1046     layer_outputs = layer_module(hidden_states, output_attentions=output_attentions)\n   1048 hidden_states = layer_outputs[0]\n   1050 if output_attentions:\n\nFile ~/miniconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1511, in Module._wrapped_call_impl(self, *args, **kwargs)\n   1509     return self._compiled_call_impl(*args, **kwargs)  # type: ignore[misc]\n   1510 else:\n-&gt; 1511     return self._call_impl(*args, **kwargs)\n\nFile ~/miniconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1520, in Module._call_impl(self, *args, **kwargs)\n   1515 # If we don't have any hooks, we want to skip the rest of the logic in\n   1516 # this function, and just call forward.\n   1517 if not (self._backward_hooks or self._backward_pre_hooks or self._forward_hooks or self._forward_pre_hooks\n   1518         or _global_backward_pre_hooks or _global_backward_hooks\n   1519         or _global_forward_hooks or _global_forward_pre_hooks):\n-&gt; 1520     return forward_call(*args, **kwargs)\n   1522 try:\n   1523     result = None\n\nFile ~/miniconda3/lib/python3.11/site-packages/transformers/models/sam/modeling_sam.py:939, in SamVisionLayer.forward(self, hidden_states, output_attentions)\n    936     height, width = hidden_states.shape[1], hidden_states.shape[2]\n    937     hidden_states, padding_shape = self.window_partition(hidden_states, self.window_size)\n--&gt; 939 hidden_states, attn_weights = self.attn(\n    940     hidden_states=hidden_states,\n    941     output_attentions=output_attentions,\n    942 )\n    943 # Reverse window partition\n    944 if self.window_size &gt; 0:\n\nFile ~/miniconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1511, in Module._wrapped_call_impl(self, *args, **kwargs)\n   1509     return self._compiled_call_impl(*args, **kwargs)  # type: ignore[misc]\n   1510 else:\n-&gt; 1511     return self._call_impl(*args, **kwargs)\n\nFile ~/miniconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1520, in Module._call_impl(self, *args, **kwargs)\n   1515 # If we don't have any hooks, we want to skip the rest of the logic in\n   1516 # this function, and just call forward.\n   1517 if not (self._backward_hooks or self._backward_pre_hooks or self._forward_hooks or self._forward_pre_hooks\n   1518         or _global_backward_pre_hooks or _global_backward_hooks\n   1519         or _global_forward_hooks or _global_forward_pre_hooks):\n-&gt; 1520     return forward_call(*args, **kwargs)\n   1522 try:\n   1523     result = None\n\nFile ~/miniconda3/lib/python3.11/site-packages/transformers/models/sam/modeling_sam.py:846, in SamVisionAttention.forward(self, hidden_states, output_attentions)\n    841 if self.use_rel_pos:\n    842     attn_weights = self.add_decomposed_rel_pos(\n    843         attn_weights, query, self.rel_pos_h, self.rel_pos_w, (height, width), (height, width)\n    844     )\n--&gt; 846 attn_weights = torch.nn.functional.softmax(attn_weights, dtype=torch.float32, dim=-1).to(query.dtype)\n    848 attn_probs = nn.functional.dropout(attn_weights, p=self.dropout, training=self.training)\n    850 attn_output = (attn_probs @ value).reshape(batch_size, self.num_attention_heads, height, width, -1)\n\nFile ~/miniconda3/lib/python3.11/site-packages/torch/nn/functional.py:1860, in softmax(input, dim, _stacklevel, dtype)\n   1858     ret = input.softmax(dim)\n   1859 else:\n-&gt; 1860     ret = input.softmax(dim, dtype=dtype)\n   1861 return ret\n\nKeyboardInterrupt:",
    "crumbs": [
      "Dataset preparation from HF"
    ]
  },
  {
    "objectID": "upload_data_hf.html",
    "href": "upload_data_hf.html",
    "title": "Hugging Face data download is here",
    "section": "",
    "text": "data is uploaded in huggingface format and here we can download it and visualize it\n\n\ndataset = load_dataset(\"hasangoni/Electron_microscopy_dataset\")\n\nDownloading and preparing dataset parquet/hasangoni--Electron_microscopy_dataset to /home/hasan/.cache/huggingface/datasets/hasangoni___parquet/hasangoni--Electron_microscopy_dataset-b8bc12bac50b90d4/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec...\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDataset parquet downloaded and prepared to /home/hasan/.cache/huggingface/datasets/hasangoni___parquet/hasangoni--Electron_microscopy_dataset-b8bc12bac50b90d4/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec. Subsequent calls will reuse this data.\n\n\n\n\n\n\ndataset\n\nDatasetDict({\n    train: Dataset({\n        features: ['image', 'label'],\n        num_rows: 1642\n    })\n    test: Dataset({\n        features: ['image', 'label'],\n        num_rows: 1725\n    })\n})\n\n\n\nsource\n\nshow_hf_dataset\n\n show_hf_dataset (dataset:datasets.arrow_dataset.Dataset,\n                  idx:Optional[int]=None, split:str='train')\n\nShow hugging face random index\n\nshow_hf_dataset(dataset)\n\n dataset index will be visualized: 300",
    "crumbs": [
      "Hugging Face data download is here"
    ]
  }
]